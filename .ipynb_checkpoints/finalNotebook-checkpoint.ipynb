{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data Analytics Final\n",
    "\n",
    "## Pubmed Central Topic Visualization\n",
    "\n",
    "### Leonardo Neves \n",
    "### AndrewID: lneves\n",
    "\n",
    "For this task, I have decided to divide my problem into some subtasks:\n",
    "\n",
    "#### 1. Preprocessing\n",
    "For the preprocessing part, that will be done offline, I will tokenize the documents, remove, stopwords, punctuation, numbers and special characters and stem the words. I will them store them back into a file.\n",
    "#### 2. TF-IDF\n",
    "Here I will calculate the TF-IDF of all files and documents. I plan on using this metric as a measure of importance when making the term correlations. Instead of summing each co-ocurrence as one, I plan on summing the TF-IDF values, so that my related terms have more chances of being relevant for the document and being a topic.\n",
    "\n",
    "Here we make the assumption that terms with higher TF-IDF would have a higher probability of being a topic.\n",
    "#### 3. Input handling\n",
    "By having computed the TF-IDF, we can work on handling the input. The user input will be preprocessed like the data we have and we will look for all the documents which the input term occur. From those documents, we will find the other terms that co-occur with this term and rank them by the sum of the the tf-idf values. From this, we have the 10 top ocurrences per term. We can now do the same for this 10 terms to find the second level of co-ocurrences and build the visualization.\n",
    "\n",
    "#### 4. Data Visualization\n",
    "From the coocurrences we find, we will generate the flare.json and create the visuzalization using D3.\n",
    "\n",
    "\n",
    "### Step 1 - Preprocessing\n",
    "\n",
    "For the preprocessing part, I have used the nltk library to create functions that would tokenize, remove stopwords and special characters and stem the words.  joblib was used to make this process parallel and use all the cores I have to compute the results. For the stopwords problem, we have seen some research related stopwords such as et, al and fig. A list with those stopwords was created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Same code as in preprocess.py'''\n",
    "import os,fnmatch\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def process_term(term):\n",
    "    '''Process an input term. Throws ValueError exception if term could not be processed.'''\n",
    "    processed_list = process_tokens([term.lower()])\n",
    "    if len(processed_list) == 0:\n",
    "        raise ValueError('Input argument is a stopword or does not exist on the corpus')\n",
    "    return processed_list[0]\n",
    "\n",
    "def process_tokens(tokens):\n",
    "    '''Treats a list of tokens, removing stopwords, special chars and stemming the terms. \n",
    "    For single term, use process_term'''\n",
    "    \n",
    "    research_stopwords = set([\"et\",\"al\",\"fig\",\"nm\",\"imag\"])\n",
    "    \n",
    "    no_special_and_stopwords = [re.sub('[^A-Za-z0-9]+', '', word) for word in tokens if (word not in stopwords.words('english') \n",
    "                                                                                         and word not in research_stopwords)]\n",
    "    #stem the terms\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    return[porter_stemmer.stem(word) for word in no_special_and_stopwords if word != \"\"]\n",
    "\n",
    "def process_file(filepath,f):\n",
    "    '''For each file, creates a new file with tonekized and stemmed words.'''\n",
    "    #open original file and create new file\n",
    "    current = open(filepath + os.sep + f,\"r\")\n",
    "    proc_file = open(os.getcwd() + os.sep +\"processedDocuments\"+os.sep + \"process_\"+f,\"w\")\n",
    "    #tokenize the words and remove tabs and eol\n",
    "    tokens = [word.strip(\"\\n\").strip(\"\\t\").strip() for word in word_tokenize(current.read().decode(\"utf8\")) if word.strip(\"\\n\").strip(\"\\t\").strip()!= \"\"]\n",
    "    #preprocess tokens\n",
    "    stemmed_words = process_tokens(tokens)\n",
    "    #persist changes to disk\n",
    "    proc_file.write(\" \".join(stemmed_words))\n",
    "    proc_file.close()\n",
    "    current.close()\n",
    "    \n",
    "def preprocess_current_folder():\n",
    "    #if output folder does not exist, creates it\n",
    "    if not os.path.exists(os.getcwd() + os.sep +\"processedDocuments\"):\n",
    "        os.makedirs(os.getcwd() + os.sep +\"processedDocuments\")\n",
    "    #Walk through all folders and subfolders looking for txt files\n",
    "    for folder in os.listdir(os.getcwd()):\n",
    "        #skip the folder with processed documents\n",
    "        if folder == \"processedDocuments\":\n",
    "            continue\n",
    "        if os.path.isdir(folder) and not folder.startswith(\".\"):\n",
    "\n",
    "            for subdir, dirs, files in os.walk(os.getcwd() + os.sep + folder):\n",
    "\n",
    "                for dir in dirs:\n",
    "                    filepath = subdir + os.sep + dir\n",
    "                    #parallel for to handle multiple files at once\n",
    "                    Parallel(n_jobs=-1)(delayed(process_file)(filepath,f) for f in os.listdir(filepath) if f.endswith(\".txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Step 2 - TF-IDF\n",
    "From the extracted values using this python code, I have adapted a code I have built from a previous class to compute TF-IDF using Hadoop. For this code, I have created five passes of mapreduce. The first just compute a word frequency for each term. The second computes the per document word count. The result from the first two passes is used on the third pass to compute the TF-IDF and, in the fourth pass, we just sort the values per tf-idf and store them in Cassandra. \n",
    "\n",
    "The fifth pass was created before I had the idea of using Cassandra to optimize the process, so it would create a map from each word to the documents they show up. After I have finished this part, I realized that using Cassandra with the proper indexes would allow me to get the two queries I wanted, the documents a word appears and the words on a document, much easier and faster, since the hardwork would be done by the optimized Cassandra CQL.\n",
    "\n",
    "The code for this part can be found on the java_code folder. Please read the readme instructions to run it.\n",
    "\n",
    "The code inside this jar will create a Cassandra keyspace and the necessary column family for the data to be stored. From this, we will be able to run our python code on the output. Be aware that the mapreduce passes create temporary results on the results folder, please do not move those files before the job is complete.\n",
    "\n",
    "### Step 3 - Input Handling\n",
    "\n",
    "For this step, we will create the functions to perform the queries from cassandra. We also create the function to, given a term, create the flare.json for the visualization part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "import json\n",
    "#Code similar to create_visualization.py\n",
    "def process_term(term):\n",
    "    '''Process an input term. Throws ValueError exception if term could not be processed.'''\n",
    "    processed_list = process_tokens([term])\n",
    "    if len(processed_list) == 0:\n",
    "        raise ValueError('Input argument is a stopword or does not exist on the corpus')\n",
    "    return processed_list[0]\n",
    "\n",
    "def connectToCluster():\n",
    "    KEYSPACE = \"big_data_final\"\n",
    "    cluster = Cluster(protocol_version=2)\n",
    "    session = cluster.connect(KEYSPACE)\n",
    "    return session\n",
    "\n",
    "def getDocuments(term,session):\n",
    "    query = \"select document FROM doc_word_map where word = '%s' ;\"%term\n",
    "    rows = session.execute(query)\n",
    "    return [row.document for row in rows]\n",
    "\n",
    "def getWordsAndTFIDF(document,session):\n",
    "    query = \"select word, tfidf from doc_word_map where document = '%s';\"%document\n",
    "    rows = session.execute(query)\n",
    "    return [(row.word,row.tfidf) for row in rows]\n",
    "def generate_json(term,session, words_per_layer = 5,parent_docs = [],level = 1,layers = 2,seen_set = set()):\n",
    "    print term\n",
    "    if level == 0:\n",
    "        process_term(term)\n",
    "    docs = getDocuments(term,session)\n",
    "    if len(docs) == 0:\n",
    "         raise ValueError('Input argument is a stopword or does not exist on the corpus')\n",
    "    occ_map = {}\n",
    "    #only get documents where they co-occur, so each level always co-occurs with previous ones\n",
    "    if len(parent_docs) > 0:\n",
    "        docs = list(set(docs) & set(parent_docs))\n",
    "    #add word to seen_set\n",
    "    seen_set.add(term)\n",
    "    research_stopwords = set([\"et\",\"al\",\"fig\",\"nm\",\"pdb\",\"imag\"])\n",
    "    for doc in docs:\n",
    "        word_tfidf =  getWordsAndTFIDF(doc,session)\n",
    "        for w in word_tfidf:\n",
    "            if w[0] in seen_set or w[0] in research_stopwords:\n",
    "                continue\n",
    "            if not w[0] in occ_map:\n",
    "                occ_map[w[0]] = 0.0\n",
    "            occ_map[w[0]] += w[1]\n",
    "        \n",
    "    order = sorted(occ_map, key= occ_map.get)[::-1]\n",
    "    flare_dict = {}\n",
    "    flare_dict[\"name\"] = term\n",
    "    flare_dict[\"children\"] = []\n",
    "    \n",
    "    for w in order[:words_per_layer]:\n",
    "        \n",
    "        if level == layers:\n",
    "            level_dict = {}\n",
    "            level_dict[\"name\"] = w\n",
    "            level_dict[\"size\"] = occ_map[w]\n",
    "            flare_dict[\"children\"].append(level_dict)\n",
    "        else:\n",
    "            level_dict = generate_json(w,session,words_per_layer = words_per_layer,parent_docs = docs,level = level+1,layers = layers,seen_set = seen_set)\n",
    "            level_dict[\"size\"] = occ_map[w]\n",
    "            flare_dict[\"children\"].append(level_dict)\n",
    "    #after full chain related to word has been computed, remove word from seen_set\n",
    "    seen_set.remove(term)\n",
    "    if level == 1:\n",
    "        with open('flare.json', 'w') as outfile:\n",
    "            json.dump(flare_dict, outfile)\n",
    "    \n",
    "    else:\n",
    "        return flare_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating the methods, we create a command line interface for the user to input the term. Here, the user can input the term, the number of co-occurrences per term and the number of layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please input the term you want to visualize: \n"
     ]
    }
   ],
   "source": [
    "session = connectToCluster()\n",
    "def get_user_input():\n",
    "    while True:\n",
    "        try:\n",
    "            term = raw_input(\"Please input the term you want to visualize: \")\n",
    "            if term == \"\":\n",
    "                break\n",
    "            words_per_layer = input(\"Please input the numbers of co-occurrences per term: \")\n",
    "            layers = input(\"Please input the numbers of layers for the visualization: \")\n",
    "\n",
    "            generate_json(term,session,layers = layers,words_per_layer = words_per_layer)\n",
    "            print \"Created Json!\"\n",
    "            break\n",
    "        except ValueError:\n",
    "            print \"Input argument is a stopword or does not exist on the corpus, please try again\"\n",
    "get_user_input()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "def show_plot(file_path,w,h):\n",
    "    with open(file_path,'r') as f:\n",
    "        s=f.read()\n",
    "    return IFrame('files/'+file_path,w,h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "I have faced difficulties with the size of the data. My computer do not have disk space to hold all the data, what makes even the sampling difficult, since I can't sample from a big number of folders. Also, even by making preporcessing parallel, it still takes several hours to process a reasonable amount of documents. On the cluster, joblib did not work and the preprocessing was not parallel, what took a really long time. I have also faced problems with Java heapspace on my computer and regarding the splitsize on the cluster, mostly because of the amount of data.\n",
    "\n",
    "### Step 4 - Visualization\n",
    "\n",
    "For our first test, we computed our flare.json with 10 terms for reach word and 4 layers. We got the following result: (for this one, I will be showing a picture because it makes the notebook to be slow when rendered).\n",
    "<img src=\"html_final/1000.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "It is clear that the image has too much information. Also, there are some repetitions within the same chain. Protein -> gene -> protein is not as informative as we expected since it is completely redundant. We will filter those kinds of situations.\n",
    "\n",
    "In addition, words like et, al, pdb(As I Googled, it is a database for proteins. Not sure if we should filter this one) and image should also be considered domain stopwords and will be filtered by our scripts. \n",
    "\n",
    "On an interactive map, however, this number of terms is not so bad. We can zoom in for the terms we are looking for and, as my json also carries the size of each term co-occurrence confidence, analyze their contribution. We can see an example here:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"600\"\n",
       "            height=\"900\"\n",
       "            src=\"files/html_final/interactive_1000.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x10db4d6d0>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_plot(\"html_final/interactive_1000.html\",600,900)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Visualization might not be accurate since it was only run with around 13k documents. With a larger corpus, the results could be different.\n",
    "\n",
    "We now try a smaller number, having a 4 co-occurences per term and only 2 layers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"900\"\n",
       "            height=\"800\"\n",
       "            src=\"files/html_final/4_2_dendogram.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x10db4d5d0>"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_plot(\"html_final/4_2_dendogram.html\",900,800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "This looks better. We can see that we still find some branches to repeate the words of other branches, but no more stopwords are shown and we do not have repetition on the same branch. Let's try a different type of visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"900\"\n",
       "            height=\"800\"\n",
       "            src=\"files/html_final/4_2_circle.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x10db4df50>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_plot(\"html_final/4_2_circle.html\",900,800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an interesting plot, since we can check each layer and the contribution for each layer. I have tried it with the 4 layers experiment, but it gets really slow because of the number of terms. Let's try for a different word as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"900\"\n",
       "            height=\"800\"\n",
       "            src=\"files/html_final/6_2_dna.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x10db4d450>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_plot(\"html_final/6_2_dna.html\",900,800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have looked into dna as the input term. Instead of 4 terms per word, I have decided to look into 6, in order to understand how many terms would be reasonable for this visualization. This is pretty interesting, but has the drawback of the words being rotated. A visualization that I found really interesting was this tree one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"900\"\n",
       "            height=\"800\"\n",
       "            src=\"files/html_final/tree2.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x10db4dd50>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_plot(\"html_final/tree2.html\",900,800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this one, although it is not possible to know the confidence level of each word, we are able to expand exactly on each branch alone, making it easier to follow the co-occurences.\n",
    "\n",
    "By searching for the terms on Google, it seems that all of the terms I found make sense. For only having a smaller number of documents, the repetitions are expected. Furthermore, I was expecting to have uninteresting words on this stage and was planning of trying POSTagging on the initial input and only get the nouns and verbs to the output.  Since I am already only having nouns and verbs as results, I will not pursue this experiment.\n",
    "\n",
    "Let's now run an example on a subsample of 15k documents. I didn't do it before because it took a long time to run, since the cluster do not allow me to use joblib for the preprocessing and I can't have all the folder to sample on my personal machine. This new subset has 3 documents from each folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please input the term you want to visualize: \n"
     ]
    }
   ],
   "source": [
    "get_user_input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"900\"\n",
       "            height=\"800\"\n",
       "            src=\"files/html_final/tree1.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x10db4d710>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_plot(\"html_final/tree1.html\",900,800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that are differences among the top co-occurences. This might be because of the way sampling was done. We got only 3 documents per folder and, assuming each folder talks about a particular set of topics, we have a really small subset of documents that talk about protein. On the other hand, by using the previous sampling, we got all documents from the same folder, so the topics were more easily found for \"protein\". The drawback of the previous method was that we limited the options for the user, since they could only ask about topics from a specific set of papers. On the other hand, we now have a high number of terms that can be searched. mrsa was not available at our first sampling technique but now we can look for it (even if it has only 26 documents with the topic). We will also Try the bubble chart, to see if it helps understand the result better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please input the term you want to visualize: \n"
     ]
    }
   ],
   "source": [
    "get_user_input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"900\"\n",
       "            height=\"800\"\n",
       "            src=\"files/html_final/tree_mrsa.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1104bfd10>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_plot(\"html_final/tree_mrsa.html\",900,800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000\"\n",
       "            height=\"800\"\n",
       "            src=\"files/html_final/bubbles.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x10db4dc50>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_plot(\"html_final/bubbles.html\",1000,800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By reading on the topic, I found that all the terms make absolute sense. We might be missing some important terms, but this could be because we do not have enough papers on the subject for the important terms to appear often.\n",
    "\n",
    "The bubble chart is interesting because we can see the confidence level of each topic. Clearly, the term smt19969, which is an antimicrobial agent, is more of a trending topic than pji, which is a specific infection that can be caused by MRSA. Probably, because the smt19969 is a common treatment, it appears on more documents (and is important for each of those documents) than pji, that probably is the subject of a fewer number of papers. Despite of that, it is really confusing to understand the relation between the words. It would be better to stick with our two visualizations, the expanding tree and the sunburst interactive visualization.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "This was a really interesting project for us to understand how dealing with huge amounts of data might make our problems much more difficult. It was able to understand some limitations of our tools, such as Hadoop and Cassandra. Hadoop was not able to perform well on a single machine, even if it is a powerful machine like the cluster. This is because it is supposed to work on commodity hardware and the default configurations are made for each datanode to do small work, rather than creating a lot of work for a single node. Cassandra is the same thing, it has limitations of the size of a query result on a single node, what made it harder for me to use the cluster and handle more data.\n",
    "\n",
    "Understanding how to handle text data was also really interesting. I put into practice concepts like stemming, tokenization, POSTagging (my first experiment was using it, but it was too simple to be here), stopwords removal ( and not only the default ones, but the ones related to the domain) and character removal.\n",
    "\n",
    "Using Cassandra to do what Cassandra does better, queries, saved me a lot of effort on coding the mappings and probably made the computation to be more efficient.  Using Hadoop for the TF-IDF might not have brought me the most efficient program, but I wanted to know better about writting map-reduce and Hadoop to Cassandra integration. I think, from the learning perspective, this was better than looking for implementations of TF-IDF.\n",
    "\n",
    "My code is organized like this. For the Hadoop part, the project is inside the folder java_code. The preprocess.py and create_visualization.py scripts are inside python_code. create_visualization.py will create the flare.json file and, if there is an HTTPServer running on http://127.0.0.1:8080, it will open two visualizations on your browser.\n",
    "\n",
    "More information on running the code on the readme.md file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
